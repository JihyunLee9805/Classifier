{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASIC MODEL- CATEGORY 1\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import logging\n",
    "\n",
    "celsisus_q=np.array([-40,-10,0,8,15,22,38])\n",
    "fahrenheit_a=np.array([-40,-14,32,46,59,72,100])\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(1,input_shape=[1]))\n",
    "\n",
    "model.compile(loss='mse',optimizer=keras.optimizers.Adam(0.1))\n",
    "his=model.fit(celsisus_q,fahrenheit_a,epochs=1000)\n",
    "\n",
    "model.save(\"./basic_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IRIS- CATEGORY 2\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data=load_iris()\n",
    "X,y=data[\"data\"],data[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "enc = OneHotEncoder()\n",
    "y_train_label = enc.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_label = enc.fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(128,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(64,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(32,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(16,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(3,activation=\"softmax\"))\n",
    "\n",
    "checkpoint_path = './iris_model.ckpt'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True,\n",
    "                                                save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"acc\"])\n",
    "model.fit(X_train,y_train_label,epochs=20,callbacks=[checkpoint],validation_data=(X_test,y_test_label))\n",
    "model.save(\"./iris_model.h5\")\n",
    "model.load_weights(checkpoint_path)\n",
    "model.evaluate(X_test,y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST- CATEGORY 2\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist=keras.datasets.mnist\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) =mnist.load_data()\n",
    "x_train=x_train/255.0\n",
    "x_test=x_test/255.0\n",
    "\n",
    "print(x_train.shape)\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "model.add(keras.layers.Dense(128,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10,activation=\"softmax\"))\n",
    "\n",
    "checkpoint_path = './mnist_model.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True,\n",
    "                                                save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "his=model.fit(x_train,y_train,epochs=15,validation_data=(x_test,y_test),callbacks=[checkpoint])\n",
    "model.save(\"./mnist_model.h5\")\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FASHION MNIST- CATEGORY 2\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import  numpy as np\n",
    "\n",
    "mnist=keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train=x_train.astype(np.float32)/255\n",
    "x_test=x_test.astype(np.float32)/255\n",
    "x_train=x_train.reshape(-1,28,28,1)\n",
    "x_test=x_test.reshape(-1,28,28,1)\n",
    "\n",
    "print(x_train.shape)\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(64,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=[28,28,1]))\n",
    "model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Conv2D(128,(3,3),activation=\"relu\"))\n",
    "model.add(keras.layers.MaxPooling2D(2,2))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10,activation=\"softmax\"))\n",
    "\n",
    "checkpoint_path = './fashion_mnist_model.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True,\n",
    "                                                save_best_only=True, monitor='val_loss', verbose=1)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "his=model.fit(x_train,y_train,epochs=15,callbacks=[checkpoint])\n",
    "model.evaluate(x_test,y_test)\n",
    "model.save(\"./fashion_mnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HORSE OR HUMAN-CATEGORY 3\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import urllib\n",
    "import zipfile\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.applications as applications\n",
    "\n",
    "#dataset,info=tfds.load(name='horses_or_humans',split=tfds.Split.TRAIN,with_info=True)\n",
    "#test_dataset=tfds.load(name='horses_or_humans',split=tfds.Split.TEST)\n",
    "\n",
    "url_train=\"https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip\"\n",
    "url_test=\"https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip\"\n",
    "\n",
    "urllib.request.urlretrieve(url_train,'horse-or-human.zip')\n",
    "local_zip='horse-or-human.zip'\n",
    "zip_ref=zipfile.ZipFile(local_zip,'r')\n",
    "zip_ref.extractall('tmp/house-or-human/')\n",
    "zip_ref.close()\n",
    "\n",
    "urllib.request.urlretrieve(url_test,'testdata.zip')\n",
    "local_zip='testdata.zip'\n",
    "zip_ref=zipfile.ZipFile(local_zip,'r')\n",
    "zip_ref.extractall('tmp/testdata/')\n",
    "zip_ref.close()\n",
    "\n",
    "train_dgen=ImageDataGenerator(rescale=1/255)\n",
    "valid_dgen=ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator=train_dgen.flow_from_directory('tmp/house-or-human/'\n",
    "      ,batch_size=128,class_mode='binary',target_size=(300,300))\n",
    "valid_generator=valid_dgen.flow_from_directory('tmp/testdata/'\n",
    "      ,batch_size=128,class_mode='binary',target_size=(300,300))\n",
    "\n",
    "pre_trained_vgg = applications.VGG16(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n",
    "pre_trained_vgg.trainable = False\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(pre_trained_vgg)\n",
    "model.add(keras.layers.Flatten())\n",
    "#model.add(keras.layers.Dense(64, activation='relu',kernel_regularizer = keras.regularizers.l1_l2(l1=0.001,l2=0.001))) # 추가 전 :0.97\n",
    "#model.add(keras.layers.Dropout(0.2)) # 추가 전: 0.97\n",
    "model.add(keras.layers.Dense(512, activation='relu',kernel_regularizer = keras.regularizers.l1_l2(l1=0.001,l2=0.001)))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "checkpoint_path = './best_performed_model.ckpt'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True,\n",
    "                                                save_best_only=True, monitor='val_loss', verbose=1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.02),loss='binary_crossentropy',metrics=['acc'])\n",
    "model.fit(train_generator,epochs=20,validation_data=valid_generator,callbacks=[checkpoint]) # epoch 늘리기\n",
    "\n",
    "\n",
    "model.save(\"./horse_or_human.h5\")\n",
    "model.load_weights(\"./horse_or_human.h5\")\n",
    "model.evaluate(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAT_DOG-CATEGORY 3\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import  numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\"\"\"\"!wget --no-check-certificate \\\n",
    "https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "-O /tmp/cats_and_dogs_filtered.zip\"\"\"\n",
    "\n",
    "\n",
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "\n",
    "# 기본 경로\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "train_dgen=ImageDataGenerator(rescale=1/255)\n",
    "valid_dgen=ImageDataGenerator(rescale=1/255)\n",
    "train_generator=train_dgen.flow_from_directory(train_dir\n",
    "      ,batch_size=32,class_mode='binary',target_size=(300,300))\n",
    "valid_generator=valid_dgen.flow_from_directory(validation_dir\n",
    "      ,batch_size=32,class_mode='binary',target_size=(300,300))\n",
    "\n",
    "#x_train=x_train.reshape(-1,28,28,1)\n",
    "#x_test=x_test.reshape(-1,28,28,1)\n",
    "\n",
    "\n",
    "pre_trained_vgg = keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n",
    "pre_trained_vgg.trainable = False\n",
    "\n",
    "\"\"\"model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  #tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\"\"\"\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(pre_trained_vgg)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1,activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "checkpoint_path = './cat_vs_dog_model.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True,\n",
    "                                                save_best_only=True, monitor='val_loss', verbose=1)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "his=model.fit(train_generator,epochs=10,validation_data=valid_generator,callbacks=[checkpoint])\n",
    "model.save(\"./cat_dog_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RPS-CATEGORY 3\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import  numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/rps.zip'\n",
    "urllib.request.urlretrieve(url, 'rps.zip')\n",
    "local_zip = 'rps.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('tmp/')\n",
    "zip_ref.close()\n",
    "\n",
    "base_dir = \"tmp/rps/\"\n",
    "train_dgen=ImageDataGenerator(rescale=1/255)\n",
    "valid_dgen=ImageDataGenerator(rescale=1/255)\n",
    "train_generator=train_dgen.flow_from_directory(base_dir\n",
    "      ,batch_size=32,class_mode='categorical',subset=\"training\",target_size=(300,300))\n",
    "valid_generator=valid_dgen.flow_from_directory(base_dir\n",
    "      ,batch_size=32,class_mode='categorical',subset=\"validation\",target_size=(300,300))\n",
    "\n",
    "#x_train=x_train.reshape(-1,28,28,1)\n",
    "#x_test=x_test.reshape(-1,28,28,1)\n",
    "\n",
    "\n",
    "pre_trained_vgg = keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n",
    "pre_trained_vgg.trainable = False\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(pre_trained_vgg)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(3,activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "checkpoint_path = './rps_model.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True,\n",
    "                                                save_best_only=True, monitor='val_loss', verbose=1)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "his=model.fit(train_generator,epochs=10,validation_data=valid_generator,callbacks=[checkpoint])\n",
    "model.save(\"./rps_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SACRASM - CATEGORY 4\n",
    "import json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "url=\"https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json\"\n",
    "urllib.request.urlretrieve(url,'sarcasm.json')\n",
    "\n",
    "with open('sarcasm.json','r') as d:\n",
    "  datas=json.load(d)\n",
    "\n",
    "sentences=[]\n",
    "labels=[]\n",
    "\n",
    "for data in datas:\n",
    "  sentences.append(data['headline'])\n",
    "  labels.append(data['is_sarcastic'])\n",
    "\n",
    "train_size = 21367\n",
    "train_x=sentences[:train_size]\n",
    "train_y=labels[:train_size]\n",
    "valid_x=sentences[train_size:]\n",
    "valid_y=labels[train_size:]\n",
    "\n",
    "vocab_size=1000\n",
    "oov_tok=\"<OOV>\"\n",
    "tokenizer=Tokenizer(num_words=vocab_size,oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "train_seqences=tokenizer.texts_to_sequences(train_x)\n",
    "valid_seqences=tokenizer.texts_to_sequences(valid_x)\n",
    "max_length=120\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "train_padded=pad_sequences(train_seqences,truncating=trunc_type,padding=padding_type, maxlen=max_length)\n",
    "valid_padded=pad_sequences(valid_seqences,truncating=trunc_type,padding=padding_type, maxlen=max_length)\n",
    "train_labels=np.asarray(train_y)\n",
    "valid_labels=np.asarray(valid_y)\n",
    "\n",
    "print(train_padded.shape)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, 16, input_length=max_length),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
    "        keras.layers.Dense(24, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "model.summary()\n",
    "checkpoint_path = './best_performed_model.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, \n",
    "                                                save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=['acc'])\n",
    "model.fit(train_padded,train_labels,batch_size=2,epochs=10,validation_data=(valid_padded,valid_labels),callbacks=[checkpoint])\n",
    "model.save(\"./sarcasm.h5\")\n",
    "model.load_weights(checkpoint_path)\n",
    "model.evaluate(valid_padded,valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUNSPOT WITH LAMDA(LSTM) -CATEGORY 5\n",
    "import csv\n",
    "import urllib\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url=\"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv\"\n",
    "urllib.request.urlretrieve(url,'./sunspots.csv')\n",
    "\n",
    "data=pd.read_csv(\"./sunspots.csv\")\n",
    "\n",
    "window_size = 100\n",
    "batch_size = 256\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "def train_data_pipeline(series, window_size, batch_size, shuffle_buffer_size):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    data = tf.data.Dataset.from_tensor_slices(series)\n",
    "    data = data.window(window_size+1, shift=1, drop_remainder=True)\n",
    "    data = data.flat_map(lambda w: w.batch(batch_size))\n",
    "    data = data.shuffle(shuffle_buffer_size)\n",
    "    data = data.map(lambda w: (w[:-1], w[1:]))\n",
    "    return data.batch(batch_size).prefetch(1)\n",
    "\n",
    "Sunspots = data['Monthly Mean Total Sunspot Number']\n",
    "series = np.array(Sunspots)\n",
    "time = np.arange(0, 3235)\n",
    "train = train_data_pipeline(series, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding='causal', activation='relu', input_shape=[None, 1]),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "  tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Lambda(lambda x: x * 25.0) #400\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "def test_data_pipeline(series):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    data = tf.data.Dataset.from_tensor_slices(series)\n",
    "    data = data.window(window_size, shift=1, drop_remainder=True)\n",
    "    data = data.flat_map(lambda w: w.batch(window_size))\n",
    "    data = data.batch(batch_size).prefetch(1)\n",
    "    return data\n",
    "\n",
    "test = test_data_pipeline(series)\n",
    "checkpoint_path = './sunspots_no_lambda_model.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True,\n",
    "                                                save_best_only=True, monitor='mae', verbose=1)\n",
    "model.compile(loss=\"mse\", optimizer='adam', metrics=[\"mae\"])\n",
    "history = model.fit(train, epochs=500,validation_data=test,callbacks=[checkpoint])\n",
    "model.save(\"./sunspot_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUNSPOT WITHOUT LAMDA(DNN) -CATEGORY 5\n",
    "import csv\n",
    "import urllib\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "\n",
    "url=\"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv\"\n",
    "urllib.request.urlretrieve(url,'./sunspots.csv')\n",
    "\n",
    "import csv\n",
    "time_step = []\n",
    "sunspots = []\n",
    "\n",
    "with open('./sunspots.csv') as csvfile:\n",
    "  reader = csv.reader(csvfile, delimiter=',')\n",
    "  next(reader)\n",
    "  for row in reader:\n",
    "    sunspots.append(float(row[2]))\n",
    "    time_step.append(int(row[0]))\n",
    "\n",
    "series = np.array(sunspots)\n",
    "time = np.array(time_step)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)\n",
    "\n",
    "split_time = 3000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "window_size = 60\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[1]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset\n",
    "\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, input_shape=[window_size], activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "checkpoint_path = \"./sunspots_no_lambda_model.ckpt\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True,save_best_only=True, monitor='mae', verbose=1)\n",
    "model.compile(loss=\"mse\",metrics=[\"mae\"], optimizer=tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9))\n",
    "model.fit(dataset,epochs=500,callbacks=[checkpoint])\n",
    "\n",
    "forecast=[]\n",
    "for time in range(len(series) - window_size):\n",
    "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "forecast = forecast[split_time-window_size:]\n",
    "results = np.array(forecast)[:,-1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, results)\n",
    "\n",
    "tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
